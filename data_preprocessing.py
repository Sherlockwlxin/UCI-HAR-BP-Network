"""
æ•°æ®å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€é¢„å¤„ç†UCI HARæ•°æ®é›†
"""

import numpy as np
import os


def load_data(data_path='UCI HAR Dataset'):
    """
    åŠ è½½UCI HARæ•°æ®é›†
    
    å‚æ•°:
        data_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    
    è¿”å›:
        X_train, y_train, X_test, y_test
    """
    print("=" * 60)
    print("å¼€å§‹åŠ è½½UCI HARæ•°æ®é›†...")
    print("=" * 60)
    
    # è®­ç»ƒé›†è·¯å¾„
    train_path = os.path.join(data_path, 'train')
    X_train_file = os.path.join(train_path, 'X_train.txt')
    y_train_file = os.path.join(train_path, 'y_train.txt')
    
    # æµ‹è¯•é›†è·¯å¾„
    test_path = os.path.join(data_path, 'test')
    X_test_file = os.path.join(test_path, 'X_test.txt')
    y_test_file = os.path.join(test_path, 'y_test.txt')
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [X_train_file, y_train_file, X_test_file, y_test_file]
    for file_path in required_files:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}\n"
                                    f"è¯·ç¡®ä¿æ•°æ®é›†å·²æ­£ç¡®è§£å‹åˆ° '{data_path}' ç›®å½•ä¸‹")
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ ä» {data_path} åŠ è½½æ•°æ®...")
    X_train = np.loadtxt(X_train_file)
    y_train = np.loadtxt(y_train_file)
    X_test = np.loadtxt(X_test_file)
    y_test = np.loadtxt(y_test_file)
    
    print(f"âœ“ è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬, {X_train.shape[1]} ç‰¹å¾")
    print(f"âœ“ æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬, {X_test.shape[1]} ç‰¹å¾")
    print(f"âœ“ ç±»åˆ«æ•°é‡: {len(np.unique(y_train))} ç±»")
    
    return X_train, y_train, X_test, y_test


def get_activity_names():
    """
    è·å–æ´»åŠ¨ç±»åˆ«åç§°
    
    è¿”å›:
        activity_names: ç±»åˆ«åç§°åˆ—è¡¨
    """
    return [
        'Walking',           # 1 - èµ°è·¯
        'Walking Upstairs',  # 2 - ä¸Šæ¥¼
        'Walking Downstairs',# 3 - ä¸‹æ¥¼
        'Sitting',           # 4 - å
        'Standing',          # 5 - ç«™
        'Laying'             # 6 - èºº
    ]


def standardize_data(X_train, X_test):
    """
    Z-Scoreæ ‡å‡†åŒ–
    å…³é”®æ­¥éª¤ï¼å¿…é¡»è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¦åˆ™æ¢¯åº¦æ— æ³•ä¸‹é™
    
    å…¬å¼: x' = (x - Î¼) / Ïƒ
    
    å‚æ•°:
        X_train: è®­ç»ƒé›†ç‰¹å¾
        X_test: æµ‹è¯•é›†ç‰¹å¾
    
    è¿”å›:
        X_train_std, X_test_std: æ ‡å‡†åŒ–åçš„æ•°æ®
    """
    print("\n" + "=" * 60)
    print("è¿›è¡Œæ•°æ®æ ‡å‡†åŒ– (Z-Score Normalization)...")
    print("=" * 60)
    
    # è®¡ç®—è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®
    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)
    
    # é˜²æ­¢é™¤ä»¥0
    std[std == 0] = 1.0
    
    # æ ‡å‡†åŒ–
    X_train_std = (X_train - mean) / std
    X_test_std = (X_test - mean) / std
    
    print(f"âœ“ è®­ç»ƒé›†æ ‡å‡†åŒ–å®Œæˆ")
    print(f"  - å‡å€¼èŒƒå›´: [{mean.min():.4f}, {mean.max():.4f}]")
    print(f"  - æ ‡å‡†å·®èŒƒå›´: [{std.min():.4f}, {std.max():.4f}]")
    print(f"âœ“ æµ‹è¯•é›†æ ‡å‡†åŒ–å®Œæˆ (ä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡)")
    
    return X_train_std, X_test_std


def one_hot_encode(y, num_classes=6):
    """
    One-Hotç¼–ç 
    å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotå‘é‡
    
    ä¾‹å¦‚: 1 -> [1, 0, 0, 0, 0, 0]
          3 -> [0, 0, 1, 0, 0, 0]
    
    å‚æ•°:
        y: æ ‡ç­¾æ•°ç»„ (å€¼ä»1åˆ°6)
        num_classes: ç±»åˆ«æ€»æ•°
    
    è¿”å›:
        y_onehot: one-hotç¼–ç åçš„æ ‡ç­¾
    """
    y = y.astype(int) - 1  # å°†æ ‡ç­¾ä»1-6è½¬æ¢ä¸º0-5
    y_onehot = np.zeros((len(y), num_classes))
    y_onehot[np.arange(len(y)), y] = 1
    return y_onehot


def split_validation(X_train, y_train, validation_split=0.15):
    """
    ä»è®­ç»ƒé›†ä¸­åˆ†ç¦»å‡ºéªŒè¯é›†
    
    å‚æ•°:
        X_train: è®­ç»ƒé›†ç‰¹å¾
        y_train: è®­ç»ƒé›†æ ‡ç­¾
        validation_split: éªŒè¯é›†æ¯”ä¾‹
    
    è¿”å›:
        X_train_new, y_train_new, X_val, y_val
    """
    num_samples = X_train.shape[0]
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    
    val_size = int(num_samples * validation_split)
    val_indices = indices[:val_size]
    train_indices = indices[val_size:]
    
    X_val = X_train[val_indices]
    y_val = y_train[val_indices]
    X_train_new = X_train[train_indices]
    y_train_new = y_train[train_indices]
    
    return X_train_new, y_train_new, X_val, y_val


def analyze_data_distribution(y_train, y_test):
    """
    åˆ†ææ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
    
    å‚æ•°:
        y_train: è®­ç»ƒé›†æ ‡ç­¾
        y_test: æµ‹è¯•é›†æ ‡ç­¾
    """
    print("\n" + "=" * 60)
    print("æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒåˆ†æ")
    print("=" * 60)
    
    activity_names = get_activity_names()
    
    print("\nè®­ç»ƒé›†åˆ†å¸ƒ:")
    for i in range(1, 7):
        count = np.sum(y_train == i)
        percentage = count / len(y_train) * 100
        print(f"  ç±»åˆ« {i} ({activity_names[i-1]:20s}): {count:4d} æ ·æœ¬ ({percentage:.2f}%)")
    
    print("\næµ‹è¯•é›†åˆ†å¸ƒ:")
    for i in range(1, 7):
        count = np.sum(y_test == i)
        percentage = count / len(y_test) * 100
        print(f"  ç±»åˆ« {i} ({activity_names[i-1]:20s}): {count:4d} æ ·æœ¬ ({percentage:.2f}%)")


def prepare_data(data_path='UCI HAR Dataset', validation_split=0.15):
    """
    å®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹
    
    å‚æ•°:
        data_path: æ•°æ®é›†è·¯å¾„
        validation_split: éªŒè¯é›†æ¯”ä¾‹
    
    è¿”å›:
        X_train, y_train, X_val, y_val, X_test, y_test (å‡å·²æ ‡å‡†åŒ–å’Œone-hotç¼–ç )
    """
    # 1. åŠ è½½åŸå§‹æ•°æ®
    X_train_raw, y_train_raw, X_test_raw, y_test_raw = load_data(data_path)
    
    # 2. åˆ†ææ•°æ®åˆ†å¸ƒ
    analyze_data_distribution(y_train_raw, y_test_raw)
    
    # 3. æ ‡å‡†åŒ–
    X_train_std, X_test_std = standardize_data(X_train_raw, X_test_raw)
    
    # 4. åˆ†ç¦»éªŒè¯é›†
    X_train, y_train, X_val, y_val = split_validation(
        X_train_std, y_train_raw, validation_split
    )
    
    print(f"\nâœ“ è®­ç»ƒ/éªŒè¯é›†åˆ†ç¦»å®Œæˆ:")
    print(f"  - è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"  - éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"  - æµ‹è¯•é›†: {X_test_std.shape[0]} æ ·æœ¬")
    
    # 5. One-Hotç¼–ç 
    print("\n" + "=" * 60)
    print("è¿›è¡ŒOne-Hotç¼–ç ...")
    print("=" * 60)
    y_train_onehot = one_hot_encode(y_train)
    y_val_onehot = one_hot_encode(y_val)
    y_test_onehot = one_hot_encode(y_test_raw)
    
    print(f"âœ“ æ ‡ç­¾ç¼–ç å®Œæˆ")
    print(f"  - åŸå§‹æ ‡ç­¾èŒƒå›´: [1, 6]")
    print(f"  - One-Hotç»´åº¦: {y_train_onehot.shape[1]}")
    print(f"  - ç¤ºä¾‹: æ ‡ç­¾ 1 -> {y_train_onehot[0]}")
    
    return X_train, y_train_onehot, X_val, y_val_onehot, X_test_std, y_test_onehot


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    X_train, y_train, X_val, y_val, X_test, y_test = prepare_data()
    print("\nâœ“ æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print(f"  è®­ç»ƒé›†å½¢çŠ¶: X={X_train.shape}, y={y_train.shape}")
    print(f"  éªŒè¯é›†å½¢çŠ¶: X={X_val.shape}, y={y_val.shape}")
    print(f"  æµ‹è¯•é›†å½¢çŠ¶: X={X_test.shape}, y={y_test.shape}")
